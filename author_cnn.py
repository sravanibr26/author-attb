# -*- coding: utf-8 -*-
"""Author_Cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1idASBuhR8lyytzH2jG9gfFF5G-OGfhL3

Import Packages and Load Data
"""

# Commented out IPython magic to ensure Python compatibility.
# Import packages
import numpy as np
import pandas as pd
import chardet
from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt
import string
import time

# Display plots inline
# %matplotlib inline

import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, make_scorer, confusion_matrix
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.preprocessing import LabelBinarizer
from sklearn.svm import SVC

from keras.models import Model
from keras.layers import Input, Dense, Flatten, Dropout, Embedding
from keras.layers.convolutional import Conv1D, MaxPooling1D
from keras.layers import concatenate
from keras.optimizers import Adam
from keras.preprocessing.text import one_hot
from keras.callbacks import ModelCheckpoint

from scipy import sparse, stats

# Download nltk - only need to run once
nltk.download('stopwords')

# Get encoding of data file
with open("/content/us.csv", 'rb') as file:
    print(chardet.detect(file.read()))

# Load data (uncomment relevant line)
# Local version
#data = pd.read_csv("author_data.csv", encoding="Windows-1252")

# Floydhub version
data = pd.read_csv("/content/us.csv", encoding="utf-8")
print(data.head())

# Create feature (text) and label (author) lists
text = list(data['text'].values)
author = list(data['author'].values)

print("The author dataset contains {} datapoints.".format(len(text)))

"""Data Exploration

Explore the author (labels) data
"""

# Check distribution of authors in the data
Counter(author)

"""Explore the text data"""

print(text[400])

print(text[2700])

print(text[6500])

print(text[600])

"""Calculate and examine word count/length and character count statistics:"""

# Create word count and character count lists
word_count = []
char_count = []

for i in range(len(text)):
    word_count.append(len(text[i].split()))
    char_count.append(len(text[i]))

# Convert lists to numpy arrays
word_count = np.array(word_count)
char_count = np.array(char_count)

# Calculate average word lengths
ave_length = np.array(char_count)/np.array(word_count)

def get_stats(var):
    """Print summary statistics for a variable of interest.

    Args:
    var: array. Numpy array containing values for the variable of interest.

    Returns:
    None
    """
    print("Min:", np.min(var))
    print("Max:", np.max(var))
    print("Mean:", np.mean(var))
    print("Median", np.median(var))
    print("1st percentile", np.percentile(var, 1))
    print("95th percentile", np.percentile(var, 95))
    print("99th percentile", np.percentile(var, 99))
    print("99.5th Percentile", np.percentile(var, 99.5))
    print("99.9th Percentile", np.percentile(var, 99.9))

print("Word count statistics")
get_stats(word_count)

# Plot word count distribution
sns.distplot(word_count, kde = False, bins = 70, color = 'blue').set_title("Word Count Distribution")
plt.xlabel('Excerpt Length (Words)')
plt.ylabel('Count')
plt.xlim(0, 100)
plt.savefig("word_count.eps")

print("\nCharacter count statistics")
get_stats(char_count)

# Plot character count distribution
sns.distplot(char_count, kde = False, bins = 100, color = 'blue').set_title("Character Count Distribution")
plt.xlabel('Excerpt Length (Characters)')
plt.ylabel('Count')
plt.xlim(0, 400)
plt.savefig("char_count.eps")

print("\nAverage length statistics")
get_stats(ave_length)

# Plot average excerpt length distribution
sns.distplot(ave_length, kde = False, bins = 70, color = 'blue').set_title("Average Word Length Distribution")
plt.xlabel('Average Excerpt Length (Characters)')
plt.ylabel('Count')
plt.xlim(0, 10)
plt.savefig("ave_length.eps")

# Get word count outliers
word_outliers = np.where(word_count > 150)

for i in word_outliers[0]:
    print("Excerpt {} - Length: {}".format(i, word_count[i]))
    print(text[i], "\n")

word_outliers = np.where(word_count < 2)

for i in word_outliers[0]:
  print("Excerpt {} - Length: {}".format(i, word_count[i]))
  print(text[i], "\n")

# Get average length outliers
length_outliers = np.where(ave_length > 10)

for i in length_outliers[0]:
    print("Excerpt {} - Average Length: {}".format(i, ave_length[i]))
    print(text[i], "\n")

length_outliers = np.where(ave_length < 3.5)

for i in length_outliers[0]:
    print("Excerpt {} - Average Length: {}".format(i, ave_length[i]))
    print(text[i], "\n")

"""Explore the words and characters"""

# Create string containing all excerpts in lower case
text_string = ''

for i in range(len(text)):
    text_string += text[i].lower()

# Get character frequencies
char_cnt = Counter(text_string)
print(char_cnt)
print(len(char_cnt))

# Get character count dictionary keys
print(list(char_cnt.keys()))

# Create list of accented characters and irrelevant characters
accented_chars = ['ï', 'é', 'ñ', 'è', 'ö', 'æ', 'ô', 'â', 'á', 'à', 'ê', 'ë','€','œ','€™','€˜','*','/','{','}','Ã','Â']

# Find all texts containing unusual characters
accented_text = []

for i in range(len(text)):
    for j in text[i]:
        if j in accented_chars:
            accented_text.append(i)

accented_text = list(set(accented_text))

print('There are', str(len(accented_text)), 'texts containing accented characters.')

# Print accented texts
for i in accented_text:
    print("Excerpt {}".format(i))
    print(text[i] + '\n')

"""Data Preprocessing

Remove invalid characters and large blocks of white space
"""

# Remove invalid character from text
for x in accented_chars:
  text = [excerpt.replace(x, '') for excerpt in text]

# Verify character has been removed
unusual_text = []

for i in range(len(text)):
    for j in text[i]:
        if j == accented_chars:
            unusual_text.append(i)

unusual_text = list(set(unusual_text))

print('There are', str(len(unusual_text)), 'texts containing the invalid character.')

# Count texts containing white space blocks
ctr = 0
for excerpt in text:
    if "  " in excerpt:
        ctr += 1

print('There are', ctr, 'excerpts containing blocks of white space.')

# Remove blocks of white space
new_text = []

for excerpt in text:
    while "  " in excerpt:
        excerpt = excerpt.replace("  "," ")
    new_text.append(excerpt)

text = new_text
print(len(text))

ctr = 0
for excerpt in text:
    if "  " in excerpt:
        ctr += 1

print('There are', ctr, 'excerpts containing blocks of white space.')

"""Remove punctuation and convert to lowercase"""

normed_text = []

for i in range(len(text)):
    new = text[i].lower()
    new = new.translate(str.maketrans('','', string.punctuation))
    new = new.replace('“', '').replace('”', '')
    normed_text.append(new)

print(normed_text[0:5])
print(len(normed_text))

"""Create training and test subsets"""

text_train, text_test, author_train, author_test = train_test_split(normed_text, author, test_size = 0.2, random_state = 5)

# Check shapes of created datasets
print(np.shape(text_train))
print(np.shape(text_test))
print(np.shape(author_train))
print(np.shape(author_test))

"""Create n-gram sequences"""

def create_n_grams(excerpt_list, n, vocab_size, seq_size):

    n_gram_list = []

    for excerpt in excerpt_list:
        # Remove spaces
        excerpt = excerpt.replace(" ", "")

        # Extract n-grams
        n_grams = [excerpt[i:i + n] for i in range(len(excerpt) - n + 1)]

        # Convert to a single string with spaces between n-grams
        new_string = " ".join(n_grams)

        # One hot encode
        hot = one_hot(new_string, round(vocab_size*1.3))

        # Pad hot if necessary
        hot_len = len(hot)
        if hot_len >= seq_size:
            hot = hot[0:seq_size]
        else:
            diff = seq_size - hot_len
            extra = [0]*diff
            hot = hot + extra

        n_gram_list.append(hot)

    n_gram_array = np.array(n_gram_list)

    return n_gram_array

def get_vocab_size(excerpt_list, n, seq_size):
    """Calculate size of n-gram vocab

    Args:
    excerpt_list: list of strings. List of normalized text excerpts.
    n: int. Length of n-grams.
    seq_size: int. Size of n-gram sequences

    Returns:
    vocab_size: int. Size of n-gram vocab.
    """
    n_gram_list = []

    for excerpt in excerpt_list:
        # Remove spaces
        excerpt = excerpt.replace(" ", "")

        # Extract n-grams
        n_grams = [excerpt[i:i + n] for i in range(len(excerpt) - n + 1)]

        # Create list of n-grams
        gram_len = len(n_grams)
        if gram_len >= seq_size:
            n_grams = n_grams[0:seq_size]
        else:
            diff = seq_size - gram_len
            extra = [0]*diff
            n_grams = n_grams + extra

        n_gram_list.append(n_grams)

    # Flatten n-gram list
    n_gram_list = list(np.array(n_gram_list).flat)

    # Calculate vocab size
    n_gram_cnt = Counter(n_gram_list)
    vocab_size = len(n_gram_cnt)

    return vocab_size

# Determine vocab sizes

for i in range(1, 4):
    vocab_size = get_vocab_size(text_train, i, 350)
    print('Vocab size for n =', i, 'is:', vocab_size)

# Create n-gram lists

gram1_train = create_n_grams(text_train, 1, 57, 350)
gram2_train = create_n_grams(text_train, 2, 1114, 350)
gram3_train = create_n_grams(text_train, 3, 9032, 350)


gram1_test = create_n_grams(text_test, 1, 57, 350)
gram2_test = create_n_grams(text_test, 2, 1114, 350)
gram3_test = create_n_grams(text_test, 3, 9032, 350)

print(np.shape(gram1_train))
print(np.shape(gram2_train))
print(np.shape(gram3_train))

print(np.shape(gram1_test))
print(np.shape(gram2_test))
print(np.shape(gram3_test))

# Determine maximum value of n-gram encodings (this is used to set the CNN embedding dimension)
max_1gram = np.max(gram1_train)
max_2gram = np.max(gram2_train)
max_3gram = np.max(gram3_train)

print('Maximum encoding value for 1-grams is: ', max_1gram)
print('Maximum encoding value for 2-grams is: ', max_2gram)
print('Maximum encoding value for 3-grams is: ', max_3gram)

"""Create bag-of-words features"""

def process_data(excerpt_list):
    """Stem data, remove stopwords and split into word lists

    Args:
    excerpt_list: list of strings. List of normalized text excerpts.

    Returns:
    processed: list of strings. List of lists of processed text excerpts (stemmed and stop words removed).
    """
    stop_words = set(stopwords.words('english'))
    porter = PorterStemmer()

    processed = []

    for excerpt in excerpt_list:
        new = excerpt.split()
        word_list = [porter.stem(w) for w in new if not w in stop_words]
        word_list = " ".join(word_list)
        processed.append(word_list)

    return processed

# Process data subsets
processed_train = process_data(text_train)
processed_test = process_data(text_test)

print(processed_train[0:5])

# Create bag of words features
## Fit Tfidf Vectorizer
vectorizer = TfidfVectorizer(strip_accents = 'ascii', stop_words = 'english', min_df = 6)
vectorizer.fit(processed_train)

# Get size of vocabulary
print('Vocabulary size: ', len(vectorizer.vocabulary_))

# Create feature vectors
words_train = vectorizer.transform(processed_train)
words_test = vectorizer.transform(processed_test)

"""One-hot encode labels

"""

# One hot encode labels
author_lb = LabelBinarizer()

author_lb.fit(author_train)
author_train_hot = author_lb.transform(author_train)
author_test_hot = author_lb.transform(author_test)

"""Implementation

Fit the CNN
"""

# Define model architecture in keras
# Code reference: https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/
def define_model(input_len, output_size, vocab_size, embedding_dim, verbose = True,
                drop_out_pct = 0.25, conv_filters = 500, activation_fn = 'relu', pool_size = 2, learning = 0.0001):
    """Define n-gram CNN

    Args:
    input_len: int. Length of input sequences.
    output_size: int. Number of output classes.
    vocab_size: int. Maximum value of n-gram encoding.
    embedding_dim: int. Size of embedding layer.
    verbose: bool. Whether or not to print model summary.
    drop_out_pct: float. Drop-out rate.
    conv_filters: int. Number of filters in the conv layer.
    activation_fn: string. Activation function to use in the convolutional layer.
    pool_size: int. Pool size for the max pooling layer.
    learning: float. Learning rate for the model optimizer.

    Returns:
    model: keras model object.
    """
    # Channel 1
    inputs1 = Input(shape = (input_len,))
    embedding1 = Embedding(vocab_size, embedding_dim)(inputs1)
    drop1 = Dropout(drop_out_pct)(embedding1)
    conv1 = Conv1D(filters = conv_filters, kernel_size = 3, activation = activation_fn)(drop1)
    pool1 = MaxPooling1D(pool_size = pool_size)(conv1)
    flat1 = Flatten()(pool1)

    # Channel 2
    inputs2 = Input(shape = (input_len,))
    embedding2 = Embedding(vocab_size, embedding_dim)(inputs2)
    drop2 = Dropout(drop_out_pct)(embedding2)
    conv2 = Conv1D(filters = conv_filters, kernel_size = 4, activation = activation_fn)(drop2)
    pool2 = MaxPooling1D(pool_size = pool_size)(conv2)
    flat2 = Flatten()(pool2)

    # Channel 3
    inputs3 = Input(shape = (input_len,))
    embedding3= Embedding(vocab_size, embedding_dim)(inputs3)
    drop3 = Dropout(drop_out_pct)(embedding3)
    conv3 = Conv1D(filters = conv_filters, kernel_size = 5, activation = activation_fn)(drop3)
    pool3 = MaxPooling1D(pool_size = pool_size)(conv3)
    flat3 = Flatten()(pool3)

    # Merge channels
    merged = concatenate([flat1, flat2, flat3])

    # Create output layer
    output = Dense(output_size, activation = 'softmax')(merged)

    # Create model
    model = Model(inputs = [inputs1, inputs2, inputs3], outputs = output)

    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer = Adam(lr = learning), metrics=['accuracy'])

    if verbose:
        print(model.summary())

    return model

# Create the 1-gram model
gram1_model = define_model(350, 9, max_1gram + 1, 57)

# Train 1-gram CNN
gram1_model.fit([gram1_train, gram1_train, gram1_train], author_train_hot, epochs=10, batch_size=32, verbose = 1, validation_split = 0.2)

# Create the 2-gram model
gram2_model = define_model(350, 9, max_2gram + 1, 1114)

# Train 2-gram CNN
gram2_model.fit([gram2_train, gram2_train, gram2_train], author_train_hot, epochs=10, batch_size=32, verbose = 1, validation_split = 0.2)

# Create the 3-gram model
gram3_model = define_model(350, 9, max_3gram + 1, 9032)

# Train 3-gram CNN
gram3_model.fit([gram3_train, gram3_train, gram3_train], author_train_hot, epochs=10, batch_size=32, verbose = 1, validation_split = 0.2)

"""Evaluation and Validation"""

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    df_cm = pd.DataFrame(cm, index = classes,
                  columns = classes)
    sns.heatmap(df_cm, annot=True, cmap = cmap)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.title(title)

# Predict values for test set
author_pred1 = gram1_model.predict([gram1_test, gram1_test, gram1_test])



# Reverse one-hot encoding of labels
author_pred1 = author_lb.inverse_transform(author_pred1)

# Evaluate
accuracy = accuracy_score(author_test, author_pred1)
precision, recall, f1, support = score(author_test, author_pred1)
ave_precision = np.average(precision, weights = support/np.sum(support))
ave_recall = np.average(recall, weights = support/np.sum(support))
ave_f1 = np.average(f1, weights = support/np.sum(support))
confusion = confusion_matrix(author_test, author_pred1, labels = ['Charles Dickens', 'Jane Austen', 'Jonathan Swift', 'Lewis Caroll', 'Mark Twain', 'Oscar Wilde', 'Robert Louis Stevenson', 'Rudyard Kipling', 'Williams Shakespeare'])

print("Accuracy:", accuracy)
print("Ave. Precision:", ave_precision)
print("Ave. Recall:", ave_recall)
print("Ave. F1 Score:", ave_f1)
print("Confusion Matrix:\n", confusion)

# Predict values for test set
author_pred2 = gram2_model.predict([gram2_test, gram2_test, gram2_test])



# Reverse one-hot encoding of labels
author_pred2 = author_lb.inverse_transform(author_pred2)

# Evaluate
accuracy = accuracy_score(author_test, author_pred2)
precision, recall, f1, support = score(author_test, author_pred2)
ave_precision = np.average(precision, weights = support/np.sum(support))
ave_recall = np.average(recall, weights = support/np.sum(support))
ave_f1 = np.average(f1, weights = support/np.sum(support))
confusion = confusion_matrix(author_test, author_pred2, labels = ['Charles Dickens', 'Jane Austen', 'Jonathan Swift', 'Lewis Caroll', 'Mark Twain', 'Oscar Wilde', 'Robert Louis Stevenson', 'Rudyard Kipling', 'Williams Shakespeare'])

print("Accuracy:", accuracy)
print("Ave. Precision:", ave_precision)
print("Ave. Recall:", ave_recall)
print("Ave. F1 Score:", ave_f1)
print("Confusion Matrix:\n", confusion)

# Predict values for test set
author_pred3 = gram3_model.predict([gram3_test, gram3_test, gram3_test])



# Reverse one-hot encoding of labels
author_pred3 = author_lb.inverse_transform(author_pred3)

# Evaluate
accuracy = accuracy_score(author_test, author_pred3)
precision, recall, f1, support = score(author_test, author_pred3)
ave_precision = np.average(precision, weights = support/np.sum(support))
ave_recall = np.average(recall, weights = support/np.sum(support))
ave_f1 = np.average(f1, weights = support/np.sum(support))
confusion = confusion_matrix(author_test, author_pred3, labels = ['Charles Dickens', 'Jane Austen', 'Jonathan Swift', 'Lewis Caroll', 'Mark Twain', 'Oscar Wilde', 'Robert Louis Stevenson', 'Rudyard Kipling', 'Williams Shakespeare'])

print("Accuracy:", accuracy)
print("Ave. Precision:", ave_precision)
print("Ave. Recall:", ave_recall)
print("Ave. F1 Score:", ave_f1)
print("Confusion Matrix:\n", confusion)

pip install scikit-plot

import scikitplot as skplt
skplt.metrics.plot_confusion_matrix(author_test, author_pred1, normalize=True)

import scikitplot as skplt
skplt.metrics.plot_confusion_matrix(author_test, author_pred2, normalize=True)

import scikitplot as skplt
skplt.metrics.plot_confusion_matrix(author_test, author_pred3, normalize=True)

# Explore the first 100 test examples
for i in range(100):
    print('Excerpt', i, '- Actual label =', author_test[i],  'Model 1 predicted label =', author_pred1[i])
    print(text_test[i], '\n')

# Explore the first 100 test examples
for i in range(100):
    print('Excerpt', i, '- Actual label =', author_test[i],  'Model 1 predicted label =', author_pred2[i])
    print(text_test[i], '\n')

# Explore the first 100 test examples
for i in range(100):
    print('Excerpt', i, '- Actual label =', author_test[i],  'Model 1 predicted label =', author_pred3[i])
    print(text_test[i], '\n')

def calculate_averages(true, pred, text):


    correct_len_chars = []
    incorrect_len_chars = []
    correct_len_words = []
    incorrect_len_words = []


    for i in range(len(true)):
        if true[i] == pred[i]:
            correct_len_chars.append(len(text[i]))
            correct_len_words.append(len(text[i].split()))
        else:
            incorrect_len_chars.append(len(text[i]))
            incorrect_len_words.append(len(text[i].split()))

    correct_ave_chars = np.mean(correct_len_chars)
    correct_ave_words = np.mean(correct_len_words)
    incorrect_ave_chars = np.mean(incorrect_len_chars)
    incorrect_ave_words = np.mean(incorrect_len_words)

    # Conduct two sample t-test
    print('Character t-test')
    print(stats.ttest_ind(correct_len_chars, incorrect_len_chars, equal_var = False))

    print('\nWord t-test')
    print(stats.ttest_ind(correct_len_words, incorrect_len_words, equal_var = False))

    return correct_ave_chars, correct_ave_words, incorrect_ave_chars, incorrect_ave_words

# Calculate averages for Model 1
correct_ave_chars1, correct_ave_words1, incorrect_ave_chars1, incorrect_ave_words1\
= calculate_averages(author_test, author_pred1, text_test)

print('Model 1 - Average excerpt length (chars) of correct examples =', correct_ave_chars1,
        'Incorrect exampes =', incorrect_ave_chars1)


print('\nModel 1 - Average excerpt length (words) of correct examples =', correct_ave_words1,
        'Incorrect exampes =', incorrect_ave_words1)

